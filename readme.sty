# GNN for Protein Function Prediction Using Sequence Similarity Networks
### A Conceptual Overview

---

## 1. What This Project Does

The goal of this project is to predict the biological function of proteins by combining two sources of information: the protein's own sequence, and the network of other proteins it is similar to. Rather than looking at each protein in isolation, this approach treats proteins as part of an interconnected graph — a Sequence Similarity Network — and learns patterns from both the individual proteins and their relationships simultaneously.

This is relevant to the MGnify project because MGnify contains billions of metagenomic proteins, many of which have unknown functions. A graph-based approach to understanding these proteins is both scientifically meaningful and computationally tractable.

---

## 2. Core Concepts

### 2.1 Sequence Similarity Network (SSN)

A Sequence Similarity Network is a graph where each node represents a single protein sequence, and an edge is drawn between two nodes when their sequences are sufficiently similar to one another. Similarity is measured computationally — typically using alignment scores or identity percentages — and a threshold is applied so that only meaningful connections are retained. The result is a structured map of protein relationships.

The structure of this network carries biological meaning. Proteins that belong to the same evolutionary family tend to cluster together. Proteins that bridge two different clusters are often functionally versatile. Highly connected hub proteins tend to be ancient and conserved across species.

The choice of similarity threshold shapes the entire network. A loose threshold produces a dense, highly connected graph where distant relationships are visible. A strict threshold produces a sparse graph where only closely related proteins are linked. Both carry different analytical value.

### 2.2 Protein Language Models and ESM2 Embeddings

A protein language model is a neural network trained on vast quantities of protein sequences in a manner analogous to how large language models are trained on text. Just as words that appear in similar contexts acquire similar vector representations, amino acids that co-occur in similar sequence contexts acquire similar internal representations in the model.

ESM2 is Meta's state-of-the-art protein language model, trained on approximately 250 million protein sequences. When a protein sequence is passed through ESM2, the model produces a fixed-size numerical vector — an embedding — that encodes evolutionary, structural, and functional properties of that protein in a compact, machine-readable form.

These embeddings serve as the node features in the graph — they tell the model what each individual protein looks like before any graph-level information is considered.

### 2.3 Graph Neural Networks (GNNs)

A Graph Neural Network is a class of deep learning model designed to operate directly on graph-structured data. Unlike standard neural networks, which treat inputs as independent, a GNN explicitly uses the connections between nodes to enrich each node's representation.

The fundamental operation in a GNN is message passing. In each layer, every node gathers information from its immediate neighbours, combines it with its own current representation, and produces an updated representation. After multiple layers, each node has effectively absorbed information from a wider and wider neighbourhood in the graph.

In the context of protein SSNs, this means that after two layers of message passing, a protein's representation has been influenced not only by its own sequence but also by the sequences of proteins it is directly similar to, and the proteins those proteins are similar to. This is a natural and powerful way to model evolutionary context.

### 2.4 Graph Attention Networks (GAT) — The Chosen Architecture

The simplest GNN variant averages the features of all neighbours equally. However, in a protein SSN, not all neighbours are equally informative. A protein with 95% sequence identity is far more relevant than one with 30% identity.

Graph Attention Networks (GAT) address this by learning attention weights — a score for each neighbour that determines how much of its information should be incorporated. These weights are learned during training, meaning the model discovers for itself which connections matter most. This makes GAT a more biologically sensible choice than a plain averaging approach for SSNs.

---

## 3. The Full Pipeline

The project involves five sequential stages, each feeding into the next.

### Stage 1 — Data Collection

A subset of well-annotated protein sequences is downloaded from a public database such as Swiss-Prot. These sequences come with known functional labels — such as Gene Ontology (GO) terms describing molecular function, biological process, or cellular component — and taxonomic labels describing the organism of origin. These labels become the prediction targets for the model.

### Stage 2 — Similarity Computation

MMseqs2 is used to compute pairwise sequence similarities across all proteins in the dataset. MMseqs2 is chosen because it is orders of magnitude faster than classical alignment tools like BLAST while producing comparable results. The output is a table of protein pairs with their associated similarity scores, which becomes the edge list of the SSN.

### Stage 3 — Graph Construction

The similarity table is loaded into NetworkX, a Python graph library. A threshold is applied to remove low-confidence edges, and the remaining pairs are used to construct the SSN. Graph-theoretic properties — such as degree, clustering coefficient, and betweenness centrality — are computed for each node and stored as supplementary features that can later be fed into the model alongside the sequence embeddings.

### Stage 4 — Node Feature Generation

Each protein sequence is passed through the ESM2 protein language model to produce a 320-dimensional embedding vector. These vectors, optionally supplemented with handcrafted biochemical features such as molecular weight, isoelectric point, and amino acid composition, form the node feature matrix — the input the GNN will use to represent each protein before any graph-level reasoning takes place.

### Stage 5 — Training and Evaluation

The graph with its node features and labels is converted into a format compatible with PyTorch Geometric, the standard Python library for GNN development. A GAT model is trained on a labelled subset of nodes to predict either functional category or taxonomic group. The model is evaluated on held-out nodes, and a series of ablation experiments are conducted to isolate the contribution of graph structure versus sequence features alone.

---

## 4. Key Tools and Why They Were Chosen

**MMseqs2** — Protein similarity computation. Chosen for its speed (1000× faster than BLAST) and suitability for large-scale metagenomics datasets, making it the tool of choice for MGnify-scale problems.

**NetworkX** — Graph construction and analysis. A mature Python library with comprehensive support for graph-theoretic operations. Used to build the SSN and compute structural node features.

**ESM2 (via HuggingFace)** — Protein sequence embeddings. Meta's best-in-class protein language model. Provides rich, information-dense representations of protein sequences without requiring manual feature engineering.

**PyTorch Geometric** — GNN training framework. The standard library for graph deep learning in Python. Handles the complexity of batching, neighbourhood sampling, and message passing, allowing focus on model design rather than infrastructure.

---

## 5. What the Experiments Demonstrate

Rather than simply reporting a single accuracy number, the project is structured around ablation studies — controlled comparisons that isolate the contribution of individual components.

**GNN vs plain feed-forward network** — Training both on the same ESM2 features, but only the GNN sees the graph structure. This isolates exactly how much the SSN contributes to predictive performance beyond the sequence alone.

**ESM2 features vs handcrafted features** — Comparing two versions of the GNN, one using protein language model embeddings and one using manually computed biochemical properties. This demonstrates the value of the language model as a feature extractor.

**Varying the similarity threshold** — Running the full pipeline at multiple MMseqs2 cutoffs (e.g. 30%, 50%, 70% sequence identity). This shows how graph density affects model performance and reveals the optimal connectivity level.

**GAT vs simpler GNN variants** — Comparing the attention-based architecture against a standard averaging GNN. This validates whether the attention mechanism provides a meaningful improvement, which it should in this setting because neighbour importance varies considerably in SSNs.

---

## 6. Connection to Graph Theory

Several classical graph theory concepts map directly onto biological observations in SSNs, making a background in algorithmic problem-solving directly applicable to this domain.

**Connected components** correspond to protein families. Proteins that are transitively similar to one another form an isolated subgraph — a natural family grouping that emerges without any manual annotation.

**Shortest paths** represent evolutionary distance. The minimum number of edges between two proteins captures how many intermediate homologs connect them, which is a meaningful proxy for how related they are.

**Clustering coefficient** reveals structural conservation. A protein embedded in a tight cluster of mutually similar proteins is likely well-conserved and functionally constrained across species.

**Betweenness centrality** identifies functional bridges. Proteins that lie on many shortest paths between different communities often mediate interactions across protein families and are typically of high biological interest.

**Community detection** (via the Louvain or Leiden algorithm) partitions the graph into functionally coherent modules — analogous to identifying clusters in competitive programming graph problems, but with evolutionary significance attached to each cluster.

---

## 7. How This Extends the MGnify Project Scope

The MGnify internship project focuses on generating and visualising SSNs. This project takes that foundation further by asking a scientific question — can the structure of an SSN predict protein function? — and answering it with a trained machine learning model.

This matters because MGnify's 2.4 billion protein database contains vast numbers of proteins with no known function. A method that can propagate functional annotations through graph structure — from annotated proteins to their uncharacterised neighbours — would have direct scientific utility for the metagenomics community.

By building this as a standalone, reproducible project before applying, it demonstrates not just familiarity with the required tools but genuine scientific engagement with the domain — which is precisely what a research mentorship programme is looking for.

---

*GNN for Protein Function Prediction — Conceptual Overview*